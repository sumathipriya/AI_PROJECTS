{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57787fb0-be86-43e6-b204-58c97ef07099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (2.39.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-generativeai) (4.11.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71126274-d0f8-456e-bcb8-120466002b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: models/chat-bison-001\n",
      "Supported Methods: ['generateMessage', 'countMessageTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/text-bison-001\n",
      "Supported Methods: ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "----------------------------------------\n",
      "Model Name: models/embedding-gecko-001\n",
      "Supported Methods: ['embedText', 'countTextTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.0-pro-vision-latest\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-pro-vision\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-pro-latest\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-pro-001\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-pro-002\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-pro\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-latest\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-001\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-001-tuning\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-002\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-8b\n",
      "Supported Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-8b-001\n",
      "Supported Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-8b-latest\n",
      "Supported Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-8b-exp-0827\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-1.5-flash-8b-exp-0924\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.5-pro-exp-03-25\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.5-pro-preview-03-25\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.5-flash-preview-04-17\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-exp\n",
      "Supported Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-001\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-exp-image-generation\n",
      "Supported Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-lite-001\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-lite\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-lite-preview\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-pro-exp\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-pro-exp-02-05\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-exp-1206\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-thinking-exp\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/learnlm-1.5-pro-experimental\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/learnlm-2.0-flash-experimental\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemma-3-1b-it\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemma-3-4b-it\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemma-3-12b-it\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemma-3-27b-it\n",
      "Supported Methods: ['generateContent', 'countTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/embedding-001\n",
      "Supported Methods: ['embedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/text-embedding-004\n",
      "Supported Methods: ['embedContent']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-embedding-exp-03-07\n",
      "Supported Methods: ['embedContent', 'countTextTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-embedding-exp\n",
      "Supported Methods: ['embedContent', 'countTextTokens']\n",
      "----------------------------------------\n",
      "Model Name: models/aqa\n",
      "Supported Methods: ['generateAnswer']\n",
      "----------------------------------------\n",
      "Model Name: models/imagen-3.0-generate-002\n",
      "Supported Methods: ['predict']\n",
      "----------------------------------------\n",
      "Model Name: models/gemini-2.0-flash-live-001\n",
      "Supported Methods: ['bidiGenerateContent', 'countTokens']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Set your API key (replace with your actual key)\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "# List and print all available models and their supported methods\n",
    "models = genai.list_models()\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Model Name: {model.name}\")\n",
    "    print(f\"Supported Methods: {model.supported_generation_methods}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a402f934-2ebc-49b1-8bdf-a2c1a7292049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def get_gemini_completion(prompt, model=\"models/gemini-1.5-pro\"):\n",
    "    genai.configure(api_key=\"AIzaSyBpojGAbx9Gh6pMWN-VyMo4gbuUJPUpLrQ\")  # Replace with your actual API key\n",
    "\n",
    "    model = genai.GenerativeModel(model)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d863328b-eb0c-40ed-98ed-3801dc20fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d71dca-86e2-44b0-aa6b-6349ef1831e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74d0fb0-d273-429f-a07e-04aa20d970c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipollol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f3209f-2200-4bca-9deb-23ef7f18b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506364f7-2bb9-466a-b2bc-1ea05981a1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p-o-p-i-l-l-o-l\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdfbc0c3-e3c3-4339-b9f2-3cd458811e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A carrot so bright, a carrot so neat,\n",
      "Stood tall in the ground, oh what a treat!\n",
      "With a smile on his face, and a spring in his root,\n",
      "This happy young carrot, played a joyful toot!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")  # Replace with your actual key\n",
    "\n",
    "def get_completion_from_messages_gemini(messages,\n",
    "                                        model_name=\"models/gemini-1.5-pro\",\n",
    "                                        temperature=1,\n",
    "                                        max_tokens=500):\n",
    "\n",
    "    # Extract system and user/assistant messages\n",
    "    system_prompt = \"\"\n",
    "    conversation = []\n",
    "\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            system_prompt = msg[\"content\"]\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            conversation.append(f\"User: {msg['content']}\")\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            conversation.append(f\"Assistant: {msg['content']}\")\n",
    "\n",
    "    # Combine system prompt and conversation\n",
    "    full_prompt = f\"{system_prompt}\\n\\n\" + \"\\n\".join(conversation)\n",
    "\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    response = model.generate_content(\n",
    "        full_prompt,\n",
    "        generation_config={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_tokens\n",
    "        }\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "# Example usage\n",
    "messages = [\n",
    "    {'role': 'system',\n",
    "     'content': \"You are an assistant who responds in the style of Dr Seuss.\"},\n",
    "    {'role': 'user',\n",
    "     'content': \"write me a very short poem about a happy carrot\"}\n",
    "]\n",
    "\n",
    "response = get_completion_from_messages_gemini(messages, temperature=1)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c69a20bd-5b19-45fb-b2e5-d9ad55ae866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The carrot, bursting with sunshine-y orange joy, danced in the garden with its earthworm friends, dreaming of becoming a delicious cake.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")  # Replace with your actual API key\n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-pro\")\n",
    "\n",
    "prompt = \"\"\"All your responses must be one sentence long.\n",
    "\n",
    "User: write me a story about a happy carrot\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\n",
    "        \"temperature\": 1,\n",
    "        \"max_output_tokens\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d8ba52-82fc-4ae4-b528-883dd0f99905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A carrot so merry, in a garden so sunny, grew tall and grew orange, a happy, crunchy bunny!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")  # Replace with your actual API key\n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-pro\")\n",
    "\n",
    "prompt = \"\"\"You are an assistant who responds in the style of Dr Seuss.\n",
    "All your responses must be one sentence long.\n",
    "\n",
    "User: write me a story about a happy carrot\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\n",
    "        \"temperature\": 1,\n",
    "        \"max_output_tokens\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a255a0f0-3404-43a8-ba5e-3b39afa148d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A carrot so bright, a carrot so neat,\n",
      "Stood tall in the ground, oh what a treat!\n",
      "With a smile on his face, and a spring in his stem,\n",
      "\"I'm happy to grow,\" he declared with a  *gleam!*\n",
      "{'prompt_tokens': 28, 'completion_tokens': 45, 'total_tokens': 74}\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")  # Replace with your API key\n",
    "\n",
    "def get_completion_and_token_count_gemini(prompt, temperature=0.7, max_tokens=250):\n",
    "    model = genai.GenerativeModel(\"models/gemini-1.5-pro\")\n",
    "\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_tokens\n",
    "        }\n",
    "    )\n",
    "\n",
    "    content = response.text.strip()\n",
    "\n",
    "    # Token estimation: 1 token â‰ˆ 4 characters (roughly)\n",
    "    total_tokens = len(prompt + content) // 4\n",
    "    prompt_tokens = len(prompt) // 4\n",
    "    completion_tokens = len(content) // 4\n",
    "\n",
    "    token_dict = {\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'completion_tokens': completion_tokens,\n",
    "        'total_tokens': total_tokens\n",
    "    }\n",
    "\n",
    "    return content, token_dict\n",
    "\n",
    "# Build the prompt\n",
    "prompt = \"\"\"You are an assistant who responds in the style of Dr Seuss.\n",
    "\n",
    "User: write me a very short poem about a happy carrot\n",
    "\"\"\"\n",
    "\n",
    "response, token_dict = get_completion_and_token_count_gemini(prompt)\n",
    "\n",
    "print(response)\n",
    "print(token_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec09486-3c45-42b9-86a8-8c7064c90cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
