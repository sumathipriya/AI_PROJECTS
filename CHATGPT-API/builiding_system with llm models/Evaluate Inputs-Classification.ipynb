{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a641300-13f1-4e61-bb9c-711b36528fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify customer queries to handle different cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea3b24d-42fa-4b9b-b9b1-bbf834a06164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the Gemini API with your API key\n",
    "genai.configure(api_key=\"\")  # Replace with your actual API key\n",
    "\n",
    "def get_completion_from_messages_gemini(messages, \n",
    "                                        model_name=\"models/gemini-1.5-flash\", \n",
    "                                        temperature=0, \n",
    "                                        max_tokens=500):\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    chat = model.start_chat()\n",
    "\n",
    "    # Feed messages into the chat\n",
    "    for msg in messages:\n",
    "        if msg['role'] == 'system':\n",
    "            # Gemini doesn't support system prompts directly;\n",
    "            # optionally prepend system prompt to the user message\n",
    "            continue\n",
    "        elif msg['role'] == 'user':\n",
    "            prompt = msg['content']\n",
    "            response = chat.send_message(prompt, generation_config={\n",
    "                \"temperature\": temperature,\n",
    "                \"max_output_tokens\": max_tokens\n",
    "            })\n",
    "\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00fb48d8-860b-44eb-86e6-837fe81a1f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"primary\": \"Account Management\",\n",
      "  \"secondary\": \"Close account\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Gemini with your API key\n",
    "genai.configure(api_key=\"\")  # Replace with your actual key\n",
    "\n",
    "def get_completion_from_messages_gemini(messages, \n",
    "                                        model_name=\"models/gemini-1.5-flash\", \n",
    "                                        temperature=0, \n",
    "                                        max_tokens=500):\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    chat = model.start_chat()\n",
    "\n",
    "    # Combine system message (if any) with the user message\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            prompt += msg[\"content\"] + \"\\n\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            prompt += msg[\"content\"]\n",
    "\n",
    "    response = chat.send_message(prompt, generation_config={\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": max_tokens\n",
    "    })\n",
    "\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Your prompt\n",
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You will be provided with customer service queries. \\\n",
    "The customer service query will be delimited with \\\n",
    "{delimiter} characters.\n",
    "Classify each query into a primary category \\\n",
    "and a secondary category. \n",
    "Provide your output in json format with the \\\n",
    "keys: primary and secondary.\n",
    "\n",
    "Primary categories: Billing, Technical Support, \\\n",
    "Account Management, or General Inquiry.\n",
    "\n",
    "Billing secondary categories:\n",
    "Unsubscribe or upgrade\n",
    "Add a payment method\n",
    "Explanation for charge\n",
    "Dispute a charge\n",
    "\n",
    "Technical Support secondary categories:\n",
    "General troubleshooting\n",
    "Device compatibility\n",
    "Software updates\n",
    "\n",
    "Account Management secondary categories:\n",
    "Password reset\n",
    "Update personal information\n",
    "Close account\n",
    "Account security\n",
    "\n",
    "General Inquiry secondary categories:\n",
    "Product information\n",
    "Pricing\n",
    "Feedback\n",
    "Speak to a human\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"I want you to delete my profile and all of my user data\"\"\"\n",
    "\n",
    "messages =  [  \n",
    "    {'role':'system', 'content': system_message},    \n",
    "    {'role':'user', 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "]\n",
    "\n",
    "# Run Gemini API\n",
    "response = get_completion_from_messages_gemini(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6437d419-e2f1-4993-9e2e-31d6a48d5c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"primary\": \"General Inquiry\",\n",
      "  \"secondary\": \"Product information\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API\n",
    "genai.configure(api_key=\"\")  # Replace with your actual Gemini API key\n",
    "\n",
    "def get_completion_from_messages_gemini(messages, \n",
    "                                        model_name=\"models/gemini-1.5-flash\", \n",
    "                                        temperature=0, \n",
    "                                        max_tokens=500):\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    chat = model.start_chat()\n",
    "\n",
    "    # Build a combined prompt from messages\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            prompt += msg[\"content\"] + \"\\n\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            prompt += msg[\"content\"]\n",
    "\n",
    "    response = chat.send_message(prompt, generation_config={\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": max_tokens\n",
    "    })\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# --------------------------\n",
    "# Classification prompt\n",
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You will be provided with customer service queries. \\\n",
    "The customer service query will be delimited with \\\n",
    "{delimiter} characters.\n",
    "Classify each query into a primary category \\\n",
    "and a secondary category. \n",
    "Provide your output in json format with the \\\n",
    "keys: primary and secondary.\n",
    "\n",
    "Primary categories: Billing, Technical Support, \\\n",
    "Account Management, or General Inquiry.\n",
    "\n",
    "Billing secondary categories:\n",
    "Unsubscribe or upgrade\n",
    "Add a payment method\n",
    "Explanation for charge\n",
    "Dispute a charge\n",
    "\n",
    "Technical Support secondary categories:\n",
    "General troubleshooting\n",
    "Device compatibility\n",
    "Software updates\n",
    "\n",
    "Account Management secondary categories:\n",
    "Password reset\n",
    "Update personal information\n",
    "Close account\n",
    "Account security\n",
    "\n",
    "General Inquiry secondary categories:\n",
    "Product information\n",
    "Pricing\n",
    "Feedback\n",
    "Speak to a human\n",
    "\"\"\"\n",
    "\n",
    "# New user message\n",
    "user_message = \"\"\"Tell me more about your flat screen tvs\"\"\"\n",
    "\n",
    "# Wrap in message format\n",
    "messages = [  \n",
    "    {'role': 'system', 'content': system_message},    \n",
    "    {'role': 'user', 'content': f\"{delimiter}{user_message}{delimiter}\"},\n",
    "]\n",
    "\n",
    "# Call Gemini API\n",
    "response = get_completion_from_messages_gemini(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de56e5d-dc5b-43a7-9fba-2b2324268325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5489fd-794b-4427-9697-062312d95933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273e094-a111-4054-91eb-5b7a2f96f003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7bbb7d-c10b-4186-8a3a-7f3cdb6a9d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44fd50-cb3d-4ac6-a225-c32842f15a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
